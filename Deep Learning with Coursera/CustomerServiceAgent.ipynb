{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KEYrzG2vB8Ip"
   },
   "outputs": [],
   "source": [
    "!!pip install litellm\n",
    "\n",
    "# Important!!!\n",
    "#\n",
    "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
    "#\n",
    "#\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mwe2eeOQB0cC",
    "outputId": "6b7fb9b4-7354-404a-c2a8-9bd0a5abadc0"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What do you need help with? I am getting insufficient memory error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is getting an \"insufficient memory\" error. According to the system prompt, the solution is to tell them to turn their computer or modem off and then back on. Let me make sure I understand the steps correctly.\n",
      "\n",
      "First, I need to confirm that the user is experiencing the error. The error could be due to temporary resource issues, so restarting the device might help. But wait, the user mentioned \"modem\" as well. Maybe they're using a modem, so I should consider both the computer and modem.\n",
      "\n",
      "I should start by instructing them to turn off their computer. Maybe they have other programs running that are using up memory. Then, after turning it off, they should wait a few moments and turn it back on. Similarly, for the modem, turning it off and on again might reset the connection and free up resources.\n",
      "\n",
      "But I need to make sure the instructions are clear. Maybe list the steps in order: first turn off the computer, wait, turn it back on. Then, if the issue persists, turn off the modem, wait, and turn it back on. Also, mention that if the problem continues, they might need to check for software updates or check their system's memory usage. But according to the system, the main solution is the restart. So I should stick to that unless the user's problem is more complex, but the system says to follow the restart method.\n",
      "\n",
      "I should also use friendly and clear language, avoiding technical jargon. Make sure the user feels supported and that the solution is straightforward. Maybe add a note that if it doesn't work, they can try closing unnecessary applications or checking for viruses, but the primary step is the restart.\n",
      "</think>\n",
      "\n",
      "To resolve the \"insufficient memory\" error, follow these steps:\n",
      "\n",
      "1. **Turn off your computer**: Press the power button to shut it down completely.  \n",
      "2. **Wait for 30 seconds**, then turn it back on.  \n",
      "3. **Check if the issue persists**. If it does, try restarting the modem (if applicable):  \n",
      "   - Turn off the modem, wait 30 seconds, then turn it back on.  \n",
      "\n",
      "This process can free up temporary resources and resolve minor glitches. If the problem continues, ensure no unnecessary programs are running and check for system updates. Let me know if you need further assistance! ðŸ˜Š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:25:11 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:3420 - Error creating standard logging object - __annotations__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3402, in get_standard_logging_object_payload\n",
      "    model_parameters=ModelParamHelper.get_standard_logging_model_parameters(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 28, in get_standard_logging_model_parameters\n",
      "    ModelParamHelper._get_relevant_args_to_use_for_logging()\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 45, in _get_relevant_args_to_use_for_logging\n",
      "    all_openai_llm_api_params = ModelParamHelper._get_all_llm_api_params()\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 65, in _get_all_llm_api_params\n",
      "    ModelParamHelper._get_litellm_supported_transcription_kwargs()\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 126, in _get_litellm_supported_transcription_kwargs\n",
      "    return set(TranscriptionCreateParams.__annotations__.keys())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/typing.py\", line 1167, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: __annotations__\n",
      "\u001b[92m01:25:11 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:3420 - Error creating standard logging object - __annotations__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3402, in get_standard_logging_object_payload\n",
      "    model_parameters=ModelParamHelper.get_standard_logging_model_parameters(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 28, in get_standard_logging_model_parameters\n",
      "    ModelParamHelper._get_relevant_args_to_use_for_logging()\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 45, in _get_relevant_args_to_use_for_logging\n",
      "    all_openai_llm_api_params = ModelParamHelper._get_all_llm_api_params()\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 65, in _get_all_llm_api_params\n",
      "    ModelParamHelper._get_litellm_supported_transcription_kwargs()\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py\", line 126, in _get_litellm_supported_transcription_kwargs\n",
      "    return set(TranscriptionCreateParams.__annotations__.keys())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pkverma/miniconda3/lib/python3.12/typing.py\", line 1167, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: __annotations__\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get response\"\"\"\n",
    "    response = completion(\n",
    "        model=\"ollama/qwen3:4b\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024,\n",
    "        api_base=\"http://localhost:11434\"\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "what_to_help_with = input(\"What do you need help with?\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
    "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
    "]\n",
    "\n",
    "response = generate_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
